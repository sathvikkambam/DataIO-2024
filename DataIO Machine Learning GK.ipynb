{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94dc5860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7822730972196698\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.39      0.45      8614\n",
      "           1       0.83      0.90      0.86     29475\n",
      "\n",
      "    accuracy                           0.78     38089\n",
      "   macro avg       0.68      0.64      0.66     38089\n",
      "weighted avg       0.76      0.78      0.77     38089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This ML random forest model was created using a smaller subset of the Feburary month.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "#TRAIN TEST SPLIT Explanation\n",
    "# The train_test_split function from scikit-learn is used to split a dataset into two subsets: a training set \n",
    "#     and a testing (or validation) set. Allows \n",
    "#     you to train your model on one portion of the data and then test its performance on a completely unseen \n",
    "#     portion. This approach helps in evaluating the model's generalization ability — how well it performs on new,\n",
    "#     unseen data.\n",
    "# Splitting: It randomly divides the data into training and testing sets based on the ratio you specify. Common splits include 70% training/30% testing, 80% training/20% testing, etc.\n",
    "# Randomization: It randomly shuffles the data before splitting. This randomization helps in reducing bias and variance in the model training process, ensuring that the split data represents the overall dataset well.\n",
    "# Stratification: Optionally, it can stratify the split. Stratification ensures that the proportion of classes (or responses in regression) in the training and testing sets reflect those in the full dataset. This is important for maintaining a consistent distribution of classes, especially in datasets where one class might be underrepresented.\n",
    "\n",
    "\n",
    "#LABEL ENCODING explanation:\n",
    "# Label Encoding is a technique used to convert categorical text data into a numerical format so that machine learning\n",
    "# algorithms, which typically work with numeric data, can understand and process the data. \n",
    "\n",
    "#  label encoding was applied to the member_casual column, which contains categorical data indicating whether \n",
    "#     a bike share user is a member or a casual rider. This column represents a nominal categorical variable \n",
    "#     because there's no inherent order between a member and a casual rider; these are just distinct categories.\n",
    "\n",
    "# Splitting Data: The dataset was split into features (X) and the target variable (y). The features include all the relevant data points that the model uses to make predictions, while the target variable is what we're trying to predict—in this case, whether a bike share user is a member or a casual rider, which was encoded into numeric format using label encoding.\n",
    "# Applying train_test_split: The train_test_split function was then called with the features and the target variable as inputs. Additionally, parameters such as test_size (the proportion of the dataset to include in the test split) and random_state (a seed for the random number generator to ensure reproducibility) were specified.\n",
    "# Obtaining Training and Testing Sets: The function returns four arrays: training data for the features (X_train), training data for the target variable (y_train), testing data for the features (X_test), and testing data for the target variable (y_test). These are used to train the model and evaluate its performance, respectively.\n",
    "\n",
    "\n",
    "\n",
    "# PARAMETERS USED:\n",
    "# X: The features from the dataset.\n",
    "# y_encoded: The target variable (member_casual), after applying label encoding.\n",
    "# test_size=0.2: This indicates that 20% of the data will be used for testing, and the remaining 80% will be used for training the model.\n",
    "# random_state=42: This ensures that the split is reproducible; the same random split will be generated each time the code is run.\n",
    "\n",
    "\n",
    "##########################################################\n",
    "# # Initialization: A LabelEncoder object from the scikit-learn library was created. \n",
    "# This object is designed to encode labels with value between 0 and n_classes-1 where n_classes is \n",
    "# the number of distinct labels.\n",
    "# # Fitting and Transforming: The fit_transform method of the LabelEncoder object was called with \n",
    "# the member_casual column as the argument. This method automatically assigns a unique integer to each category \n",
    "# (member and casual), fits the encoder, and then transforms the textual labels into these integers.\n",
    "# # Result: After encoding, the member_casual column was transformed into a numeric column, where one \n",
    "# category (e.g., member) might be encoded as 0, and the other (e.g., casual) as 1. \n",
    "# These numeric labels were then used as the target variable for training the Random Forest classifier.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('February.csv')\n",
    "\n",
    "# Convert datetime features to numerical (duration in minutes)\n",
    "data['started_at'] = pd.to_datetime(data['started_at'])\n",
    "data['ended_at'] = pd.to_datetime(data['ended_at'])\n",
    "data['duration_minutes'] = (data['ended_at'] - data['started_at']).dt.total_seconds() / 60\n",
    "\n",
    "# Select features and target variable\n",
    "features = ['rideable_type', 'start_lat', 'start_lng', 'end_lat', 'end_lng', 'duration_minutes']\n",
    "X = data[features]\n",
    "y = data['member_casual']\n",
    "\n",
    "# Encode categorical variables and fill missing values\n",
    "X_encoded = pd.get_dummies(X, columns=['rideable_type'])\n",
    "X_filled = X_encoded.fillna(X_encoded.median())\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filled, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest classifier with fewer estimators for simplicity\n",
    "rf_classifier = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can help with seeing if they are a member or not without any user input and can automatically pop up \n",
    "# a notification that you are a member and will get a discount with another verification step.\n",
    "# Accuracy: The model has an accuracy of approximately 78.23%. This means that the model correctly \n",
    "# predicted the target variable (whether a bike share user is a member or a casual rider) 78.23% of the time\n",
    "# across the test dataset.\n",
    "# Precision and Recall:\n",
    "# For class 0 (which could represent 'casual' riders, assuming 0 was assigned to 'casual'):\n",
    "# Precision: 0.52, meaning when the model predicted 'casual', it was correct 52% of the time.\n",
    "# Recall: 0.39, indicating that out of all actual 'casual' riders, the model correctly identified 39% of them.\n",
    "# F1-Score: 0.45, which is the harmonic mean of precision and recall, giving a single score that \n",
    "# balances the two — a bit closer to recall in this case due to its lower value.\n",
    "# Support: 8614, which is the number of actual instances of 'casual' riders in the test set.\n",
    "# For class 1 (likely 'member' riders):\n",
    "# Precision: 0.83, so when the model predicted 'member', it was correct 83% of the time.\n",
    "# Recall: 0.90, meaning it correctly identified 90% of all 'member' riders.\n",
    "# F1-Score: 0.86, the harmonic mean of precision and recall for 'member' predictions.\n",
    "# Support: 29475, the number of actual instances of 'member' riders in the test set.\n",
    "# Macro Average:\n",
    "# Precision: 0.68, the average precision for both classes without taking class imbalance into account.\n",
    "# Recall: 0.64, the average recall for both classes.\n",
    "# F1-Score: 0.66, the average F1-score for both classes.\n",
    "# Weighted Average:\n",
    "# Precision: 0.76, the average precision for both classes while taking class imbalance into account.\n",
    "# Recall: 0.78, the average recall for both classes, weighted by the support of each class.\n",
    "# F1-Score: 0.77, the weighted average of the F1-score for both classes.\n",
    "# The model performs significantly better at identifying 'member' riders than 'casual' riders, \n",
    "# which is evident from the higher scores\n",
    "# in precision, recall, and F1-score for class 1. However, there is a notable imbalance in the dataset \n",
    "# (reflected in the support values), with more 'member' riders than 'casual' riders, which may affect the model's\n",
    "# performance metrics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
